# 1.1 介绍

自第一台通用电子计算机诞生以来的大约70年间，计算机技术取得了令人难以置信的进步。今天，不到500美元就能买到一部手机，其性能与1993年以5000万美元购买的世界上最快的计算机一样。这种快速的改进既来自于用于制造计算机的工艺技术的进步，也来自于计算机设计的创新。

尽管历史上的工艺技术改进是相当稳定的，但由更好的计算机架构带来的进步却不那么稳定。在电子计算机的前25年中，这两种力量都做出了重大贡献，每年提供大约25%的性能改进。20世纪70年代末，微处理器出现了。微处理器有能力驾驭集成电路技术的改进，导致了更高的性能改进率--大约每年增长35%。

这种增长速度，加上大规模生产的微处理器的成本优势，导致越来越多的计算机业务是基于微处理器的。此外，计算机市场的两个重大变化使新架构的商业成功比以往任何时候都容易。首先，汇编语言编程的实际取消（译者注：以c语言为代表的高级编程语言在大多数场合逐渐取代了汇编语言）减少了对目标代码兼容性的需求。其次，标准化的、与供应商无关的操作系统的建立，如UNIX及其衍生的Linux，降低了推出新架构的成本和风险。

这些变化使我们有可能在20世纪80年代初成功地开发出一套具有更简单指令的新架构，称为RISC（精简指令集计算机）架构。基于RISC的机器将设计者的注意力集中在两个关键的性能技术上，即利用指令级的并行性（最初通过流水线，后来通过多指令问题）和使用缓存（最初以简单的形式，后来使用更复杂的组织和优化）。

基于RISC的计算机提高了性能标准，迫使以前的架构跟上或消失。数字设备公司（Digital Equipment）的Vax无法做到，因此它被RISC架构所取代。英特尔迎接了这一挑战，主要是通过在内部将80x86指令翻译成类似RISC的指令，使其能够采用许多首先在RISC设计中开创的创新。随着90年代末晶体管数量的飙升，翻译更复杂的x86架构的硬件开销变得可以忽略不计。在低端设备中，如手机，x86转换开销的功率和硅面积成本劣势使得基于RISC架构的ARM成为主导。

图1.1显示，架构和组织改进的结合导致了 17 年来性能的持续增长，年增长率超过 50%，这在计算机行业是前所未有的。

![图1.1 该图描述了相对于VAX 11/780的程序性能，由SPEC整数基准测量（见1.8节）。在20世纪80年代中期之前，处理器性能的增长主要是由制造工艺驱动的，平均每年约22%，或者说每3.5年性能就翻一番。从1986年开始增长到约52%，或每两年翻一番，这要归功于RISC架构中典型的更先进的架构和组织思想。到2003年，这种增长导致了与继续以22%的速度增长时的性能相差约25倍。2003 年，由于 Dennard定律和可用的指令级并行化时代的结束，单处理器的性能在 2011 年之前放缓到每年 23%，或每 3.5 年翻一番。(自2007年以来，最快的SPECintbase性能已经开启了自动并行化，所以单核处理器的速度更难衡量）。这些结果仅限于单芯片系统，每个芯片通常有四个内核）。从2011年到2015年，每年的改进不到12%，或每8年翻一番，部分原因是阿姆达尔定律的并行性限制。自2015年以来，随着摩尔定律的结束，每年的改进仅为3.5%，或每20年翻一番 面向浮点计算的性能也遵循同样的趋势，但通常在每个阴影区域有1%到2%的高年增长率。图1.11显示了这些相同时代的时钟速率的改进。由于SPEC多年来发生了变化，较新机器的性能是通过一个比例系数来估计的，该系数与不同版本的SPEC的性能有关。SPEC89、SPEC92、SPEC95、SPEC2000和SPEC2006。SPEC2017的结果太少了，还无法绘制。](../.gitbook/assets/NeatReader-1655966534161.png)

在20世纪，这种急剧的增长速度产生了四方面的影响。首先，它大大提高了计算机的算力。对于许多应用，最高性能的微处理器超过了不到20年前的超级计算机。

其次，这种性价比的大幅提高导致了新一类计算机的出现。个人电脑和工作站在20世纪80年代随着微处理器的出现而面世。过去十年，智能手机和平板电脑兴起，许多人将其作为主要的计算平台，而不是PC。这些移动客户端设备越来越多地使用互联网来访问包含10万台服务器的数据仓库，这些服务器被设计得就像一台单一的巨型计算机。

第三，正如摩尔定律所预测的那样，半导体制造的工艺改进导致了基于微处理器的计算机在整个计算机设计范围内的主导地位。传统上用现成的逻辑或门阵列制造的微型计算机，被使用微处理器制造的服务器所取代。甚至大型计算机和高性能超级计算机也都是微处理器的集合。

前面的硬件创新导致了计算机设计的复兴，它既强调架构创新又强调有效利用工艺技术改进。这种增长速度是复合型的，所以到2003年，高性能微处理器的速度是单纯依靠工艺技术（包括改进的电路设计）获得的7.5倍，即每年52%，对比单纯靠工艺技术的35%。

这种硬件的复兴导致了第四个影响，即对软件开发的影响。自1978年以来，这种50,000倍的性能改进（见图1.1）使现代程序员能够以性能换取生产力。取而代之的是C和C++这样面向性能的语言，今天更多的编程是用Java和Scala这样的管理型编程语言完成的。此外，像JavaScript和Python这样的脚本语言甚至更有生产力，与AngularJS和Django这样的编程框架一起越来越受欢迎。为了保持生产力并努力缩小性能差距，带有即时编译器和基于跟踪的编译的解释器正在取代过去的传统编译器和链接器。软件部署也在发生变化，通过互联网使用的软件即服务（SaaS）取代了必须在本地计算机上安装和运行的需要安装的软件。

应用程序的性质也在变化。语音（speech）、声音（sound）、图像和视频正变得越来越重要，还有对用户体验非常关键的可预测的响应时间。一个鼓舞人心的例子是谷歌翻译。这个应用程序让你举起你的手机，把它的摄像头对准一个物体，图像就会通过互联网无线发送到一个仓库级计算机（WSC），该计算机可以识别照片中的文字并将其翻译成你的母语。你也可以对它说话，它将把你说的话翻译成另一种语言的音频输出。它可以翻译90种语言的文字和15种语言的语音。

图1.1还显示，这17年的硬件复兴已经结束。根本原因是半导体工艺的维持了几十年的两个特征，现在已经不复存在。

1974年，罗伯特-丹纳德（Robert Dennard）观察到，由于每个晶体管的尺寸较小，即使你增加了晶体管的数量，对于一定面积的硅，功率密度也是恒定的。值得注意的是，晶体管可以工作的得更快，但使用更少的功率。Dennard定律在2004年左右结束，因为电流和电压不能持续下降而仍然保持集成电路的可靠性。

这一变化迫使微处理器行业使用多个高效处理器或内核，而不是单一的低效处理器。事实上，2004 年英特尔取消了其高性能单核处理器项目，并与其他公司一起宣布，通向更高性能的道路将是通过每个芯片的多个处理器，而不是通过更快的单核处理器。这一里程碑标志着从单纯依靠指令级并行（ILP）（本书前三版的主要关注点）向数据级并行（DLP）和线程级并行（TLP）的历史性转变，这在第四版中有所体现，在第五版中有所扩展。第五版还增加了WSC和请求级并行（RLP），这在本版中得到了扩展。编译器和硬件的协同利用ILP而不需要程序员的关注（译者注：对应用层的程序员来讲是透明的），而DLP、TLP和RLP是显式并行，需要对应用程序进行重组，使其能够利用显式并行。在少数情况下，这很容易；但在许多情况下，这对程序员来说是一个重大的新负担。

阿姆达尔定律（第1.9节）规定了每个芯片有用内核数量的实际限制。如果10%的任务是串行的，那么无论你在芯片上放多少个核，并行化的最大性能收益是10。

最近结束的第二个观察是摩尔定律。1965年，戈登-摩尔（Gordon Moore）著名地预言，每块芯片的晶体管数量将每年翻一番，1975年修正为每两年一次。这一预测持续了大约50年，但不再成立。例如，在本书的2010年版中，最新的英特尔微处理器有1,170,000,000个晶体管。如果摩尔定律继续下去，我们可以预计2016年的微处理器将有18,720,000,000个晶体管。相反，相当的英特尔微处理器只有1,750,000,000个晶体管，与摩尔定律所预测的有10倍的差距。

下述事项：

* 由于摩尔定律的放缓和Dennard定律的结束，晶体管的性能不再有很大的提高。
* 微处理器的功率设计不变。
* 用几个高能效的处理器取代单个高能耗的处理器。
* 阿姆达尔定律导致的多处理的限制。

的综合导致了处理器性能的提高将放缓，即每20年翻一番，而不是像1986年至2003年期间那样每1.5年翻一番（见图1.1）。

提高能效-性能-成本的唯一途径是专业化。未来的微处理器将包括几个特定领域的内核，这些内核仅能很好地完成一类计算，但它们比通用内核做得明显更好。本版新的[第七章](../di-qi-zhang-ling-yu-te-ding-jia-gou-dsa.md)介绍了特定领域的体系结构。

关于架构思想和伴随的编译器改进，使上个世纪令人难以置信的增长速度成为可能。本书也将介绍导致这些巨大变化的原因，以及21世纪的新的架构思想、编译器和解释器的挑战和初步有希望的方法。其核心是计算机设计和分析的定量方法，以程序的经验观察、实验和模拟作为其工具。本书反映的正是这种计算机设计的风格和方法。本章的目的是为下面的章节和附录奠定定量的基础。

写这本书不仅是为了解释这种设计风格，也是为了激励你为这一进步做出贡献。我们相信这种方法将为未来的计算机服务，就像它对过去的隐式并行计算机起作用一样。

